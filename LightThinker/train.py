import os
os.environ["TOKENIZERS_PARALLELISM"] = "false" 
import argparse
import torch
from typing import *
from tqdm import tqdm
from copy import deepcopy
from model_qwen import Qwen2ForCausalLM
from model_llama import LlamaForCausalLM
from transformers import Trainer, TrainingArguments
import copy
# from torch.serialization import safe_globals
# from deepspeed.runtime.zero.config import ZeroStageEnum
# from deepspeed.runtime.fp16.loss_scaler import LossScaler
# from deepspeed.runtime.config import DeepSpeedConfig
import deepspeed
import torch.distributed as dist
from datetime import timedelta  # 引入时间库
# # 定义需要允许的全局对象
# DEEPSPEED_GLOBALS = [
#     ZeroStageEnum,
#     LossScaler,
#     DeepSpeedConfig
# ]


# ===== 关键：在任何其他操作前绑定GPU设备 =====
if "LOCAL_RANK" in os.environ:
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank) 
    print(f"Process {os.getpid()} set to device: cuda:{local_rank}")
else:
    local_rank = 0
    print("Running in single-GPU mode")
# =========================================



deepspeed.init_distributed(
    dist_backend='nccl', 
    timeout=timedelta(minutes=120) 
)

from config import Config
from LightThinker.utils import _print, IGNORE_LABEL_ID, str2bool
from tokenizer import Tokenizer
from dataset import MyDataset, MyDataCollator

from transformers import TrainerCallback

class SaveTokenizerCallback(TrainerCallback):
    """保存checkpoint同时保存tokenizer"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def on_save(self, args, state, control, **kwargs):
        """在保存checkpoint时调用"""
        checkpoint_folder = os.path.join(
            args.output_dir,
            f"checkpoint-{state.global_step}"
        )
        
        if os.path.exists(checkpoint_folder):
            self.tokenizer.save_pretrained(checkpoint_folder)

def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('--local_rank', type=int, help="just used for deepspeed.")
    parser.add_argument('--model_path', type=str)
    parser.add_argument('--tokenizer_path', type=str, default=None)
    parser.add_argument('--train_path', type=str, help='training dataset path')
    parser.add_argument('--output_dir', type=str)
    parser.add_argument('--max_length', type=int, default=768)
    parser.add_argument('--model_type', type=str, choices=['qwen', 'llama'])

    parser.add_argument('--compress_config', type=str)
    parser.add_argument('--bos_token', type=str)
    parser.add_argument('--eos_token', type=str)
    parser.add_argument('--see_current', type=str2bool)
    parser.add_argument('--bi_directional', type=str2bool)
    parser.add_argument('--diagonal', type=str2bool)
    parser.add_argument('--mode', type=str, choices=['recover', 'normal', 'aug', 'aug-wo-pc'])
    parser.add_argument('--exclude_continue', type=str2bool)
    parser.add_argument('--qkv', type=str)
    parser.add_argument('--freeze_model', type=str2bool)
    parser.add_argument('--train_on_input', type=str2bool)
    parser.add_argument('--output_compress_instruction', type=str)
    parser.add_argument('--hybrid', type=str2bool)  
    parser.add_argument('--prefill_compress', type=str2bool, default=True)

    parser.add_argument('--epochs', type=int)
    parser.add_argument('--lr', type=float)
    parser.add_argument('--save_steps', type=int)
    parser.add_argument('--deepspeed', type=str, help="file path")
    parser.add_argument('--micro_batch_size', type=int)
    parser.add_argument('--gradient_accumulation_steps', type=int)
    parser.add_argument('--warmup_ratio', type=float, default=0.)
    parser.add_argument('--warmup_steps', type=int, default=0)
    parser.add_argument('--lr_scheduler_type', type=str, default='linear')

    parser.add_argument('--use_EPL', type=str2bool, default=False)
    parser.add_argument('--use_aux_model', type=str2bool, default=False)
    args = parser.parse_args()
    return args

def get_model_and_tokenizer(
    args,
    comp_config:Config
) -> Tuple[Union[Qwen2ForCausalLM, LlamaForCausalLM], Tokenizer]:
    special_token_list:List[str] = list()
    special_token_desp_dict = dict()
    tokenizer: Tokenizer = Tokenizer(
        tokenizer_path=args.tokenizer_path if args.tokenizer_path != None else args.model_path,
        bos_token=args.bos_token,
        eos_token=args.eos_token,
        special_token_list=None,
        add_prefix_space=False,
    )
    assert len(comp_config.special_token_desp_list) == len(comp_config.special_token_name_list)
    for desp, token in zip(comp_config.special_token_desp_list, comp_config.special_token_name_list):
        if tokenizer.convert_tokens_to_ids(token) == None:
            special_token_list.append(token)
            # special_token_desp_list.append(desp)
            special_token_desp_dict[token] = desp
    if len(special_token_list) > 0:
        tokenizer.add_special_token(special_token_list)
    
    if args.model_type == 'llama':
        model_class = LlamaForCausalLM
    elif args.model_type == 'qwen':
        model_class = Qwen2ForCausalLM
    else:
        assert False, "We only support llama and qwen model."
    model = model_class.from_pretrained(
        args.model_path, torch_dtype=torch.bfloat16, use_aux_model=args.use_aux_model
    )
    if args.use_aux_model:
        model.model_aux.lm_head.load_state_dict(model.lm_head.state_dict())
    model.add_qkv(
        q='q' in args.qkv,
        k='k' in args.qkv,
        v='v' in args.qkv,
    )

    if model.model.config.vocab_size != len(tokenizer):
        # Expand the token embedding and lm_head
        _print(f"before.embedding.shape={model.model.embed_tokens.weight.shape}")
        _print(f"before.lm_head.shape={model.lm_head.weight.shape}")
        model.resize_token_embeddings(len(tokenizer), mean_resizing=False)
        _print(f"now.embedding.shape={model.model.embed_tokens.weight.shape}")
        _print(f"now.lm_head.shape={model.lm_head.weight.shape}")
        
        if args.use_aux_model:
            model.model_aux.resize_token_embeddings(len(tokenizer), mean_resizing=False)
            _print(f"now.embedding.shape={model.model_aux.model.embed_tokens.weight.shape}")
            _print(f"now.lm_head.shape={model.model_aux.lm_head.weight.shape}")
    
    if args.freeze_model:
        _print(f"Freezing Model:\nnew_token: {len(special_token_list)}\norigin_length: {len(tokenizer) - len(special_token_list)}")
        model.freeze_embed(
            new_token_cnt=len(special_token_list), 
            origin_length=len(tokenizer) - len(special_token_list)
        )
    else:
        _print("mean ...")
        with torch.no_grad():
            for idx, token in enumerate(reversed(special_token_list), start=1):
                description = special_token_desp_dict[token]
                tokenized = tokenizer.tokenize(description)
                tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)

                # embedding layer
                new_embedding = model.model.embed_tokens.weight[tokenized_ids].mean(axis=0)
                model.model.embed_tokens.weight[-idx, :] = new_embedding.clone().detach().requires_grad_(True)

                # lm_head layer
                last_embedding = model.lm_head.weight[tokenized_ids].mean(axis=0)
                model.lm_head.weight[-idx, :] = last_embedding.clone().detach().requires_grad_(True)
    
    trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]
    print("Trainable Parameters:")
    for param_name in trainable_params:
        print(param_name)

    return model, tokenizer

def get_dataset_and_data_collator(
    args,
    comp_config:Config,
    tokenizer:Tokenizer,
    padding_config:Dict,
    attention_config:Dict,
    sample_config:Dict,
) -> Tuple[MyDataset, MyDataCollator]:
    
    cache_dir=os.path.join("./data/train","dataset_cache")
    tokenizer_name = os.path.basename(os.path.normpath(args.tokenizer_path))
    dataset_name = os.path.splitext(os.path.basename(args.train_path))[0]
    cache_filename=f"cache_{tokenizer_name}_{dataset_name}.pt"
    
    dataset = MyDataset(
        file_path=args.train_path,
        config=comp_config,
        tokenizer=tokenizer,
        padding_config=padding_config,
        train_on_input=args.train_on_input,
        change_rope=False,
        output_compress_instruction=args.output_compress_instruction,
        cache_dir=cache_dir,
        cache_filename=cache_filename,
        force_preprocess=False,
        local_rank=local_rank,
        use_EPL=args.use_EPL,
    )

    data_collator = MyDataCollator(
        dataset=dataset,
        attention_config=attention_config,
        exclude_continue=args.exclude_continue,
        sample_config=sample_config
    )

    return dataset, data_collator


def main():
    args = get_parser()
    if args.output_compress_instruction == "None":
        args.output_compress_instruction = ""
    print(args)
    
    resume_from_checkpoint = None
    if os.path.exists(args.output_dir):
        checkpoints = [d for d in os.listdir(args.output_dir) if d.startswith("checkpoint-")]
        if checkpoints:
            latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split("-")[-1]))[-1]
            resume_from_checkpoint = os.path.join(args.output_dir, latest_checkpoint)
            print(f"发现检查点，将从 {resume_from_checkpoint} 恢复训练")

    comp_config = Config.from_file(config_path=args.compress_config)
    model, tokenizer = get_model_and_tokenizer(
        args, comp_config
    )

    sample_config:Dict = dict(
        mode=args.mode,
        hybrid=args.hybrid
    )
    attention_config:Dict = dict(
        diagonal=args.diagonal,
        bi_directional=args.bi_directional,
        see_current=args.see_current,
        prefill_compress=args.prefill_compress,
    )
    padding_config = dict(
        padding_side='right',
        label_padding_id=IGNORE_LABEL_ID,
        input_padding_id=tokenizer.eos_token_id,
        max_length=args.max_length,
        position_ids_padding_id=0,
    )

    dataset, data_collator = get_dataset_and_data_collator(
        args=args, 
        comp_config=comp_config,
        tokenizer=tokenizer,
        padding_config=padding_config,
        attention_config=attention_config,
        sample_config=sample_config,
    )

    training_config = TrainingArguments(
        lr_scheduler_type=args.lr_scheduler_type,
        local_rank=args.local_rank,
        gradient_checkpointing=True,
        num_train_epochs=args.epochs,
        learning_rate=args.lr,
        bf16=True,
        logging_steps=1,
        do_eval=False,
        optim="adamw_torch",
        save_strategy="epoch",      # the default value is step
        save_steps=args.save_steps, # if the strategy is epoch, the save_steps is not used.
        output_dir=args.output_dir,
        save_only_model=False,       # don't save the global_steps
        load_best_model_at_end=False,
        deepspeed=args.deepspeed,
        save_total_limit=10,
        report_to="tensorboard",
        per_device_train_batch_size=args.micro_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        warmup_steps=args.warmup_steps,
        warmup_ratio=args.warmup_ratio
    )
    trainer = Trainer(
        model=model,
        train_dataset=dataset,
        args=training_config,
        data_collator=data_collator,
        callbacks=[SaveTokenizerCallback(tokenizer)]  # 添加回调
    )
    # 在加载检查点时使用上下文管理器
    if resume_from_checkpoint:
           trainer.train(resume_from_checkpoint=resume_from_checkpoint)
    else:
        trainer.train()
    


if __name__ == '__main__':
    main()